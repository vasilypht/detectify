from pathlib import Path
from typing import Union

import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import (
    PretrainedConfig,
    PreTrainedModel,
    ResNetModel,
    LongformerModel,
    LongformerTokenizerFast,
    AutoImageProcessor,
    LongformerConfig,
    ResNetConfig,
)

import sys
sys.path.append('/Users/vasilyperekhrest/PycharmProjects/malware-detection-system')

from src import parsers
from src.extractor import VirusTotalFeatureExtractor


TOKENIZER_OPTIONS = {
    'add_special_tokens': True,
    'return_attention_mask': True,
    'max_length': 16_384,
    'padding': 'max_length',
    'truncation': True,
    'return_tensors': 'pt',
}


class MalwareDetectionConfig(PretrainedConfig):
    ...
    

class MalwareDetectionModel(PreTrainedModel):
    config_class = MalwareDetectionConfig
    
    def __init__(self, config: MalwareDetectionConfig) -> None:
        super(MalwareDetectionModel, self).__init__(config)
        self.config = config
        
        resnet_config = ResNetConfig.from_dict(config.resnet_config)
        self.resnet_model = ResNetModel(resnet_config)
        
        longformer_config = LongformerConfig.from_dict(config.longformer_config)
        self.longformer_model = LongformerModel(longformer_config)
        for param in self.longformer_model.base_model.parameters():
            param.requires_grad = False

        in_features = self.resnet_model.config.hidden_sizes[-1] + self.longformer_model.config.hidden_size

        self.linear = torch.nn.Linear(in_features, self.config.hidden_size)
        self.fc = torch.nn.Linear(self.config.hidden_size, self.config.num_classes)
        self.post_init()

    def forward(
            self,
            longformer_input_ids,
            longformer_attention_mask,
            longformer_global_attention_mask,
            resnet_pixel_values,
    ):
        resnet_pooler_output = self.resnet_model(pixel_values=resnet_pixel_values).pooler_output
        resnet_flatten_pooler_output = resnet_pooler_output.flatten(start_dim=1)

        longformer_pooler_output = self.longformer_model(
            input_ids=longformer_input_ids,
            attention_mask=longformer_attention_mask,
            global_attention_mask=longformer_global_attention_mask,
        ).pooler_output

        concated_outputs = torch.concat([resnet_flatten_pooler_output, longformer_pooler_output], dim=1)
        logits = torch.relu(self.linear(concated_outputs))
        logits = self.fc(logits)
        return logits
    

class MalwareDetectionPipeline:
    def __init__(
            self,
            model_path: Union[Path, str],
            device: Union[str, torch.device] = torch.device('cpu'),
            tokenizer_options: dict = TOKENIZER_OPTIONS,
    ) -> None:
        self.device = device if torch.cuda.is_available() else torch.device('cpu')
        self.model = MalwareDetectionModel.from_pretrained(model_path)
        self.model = self.model.to(self.device)
        
        self.tokenizer = LongformerTokenizerFast.from_pretrained(model_path)
        self.image_processor = AutoImageProcessor.from_pretrained(model_path)
        
        self.text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
            tokenizer=self.tokenizer,
            chunk_size=16_384,
            chunk_overlap=8_192,
        )
        
        self.tokenizer_options = tokenizer_options
    
    @torch.inference_mode()
    def __call__(self, file_path: Union[str, Path], text: str):
        with open(file_path, 'rb') as file:
            img_bytes = list(file.read(224 * 224))
        
        padding = [0] * (224*224 - len(img_bytes))
        img_bytes.extend(padding)
        
        img = torch.reshape(
            input=torch.tensor(img_bytes, dtype=torch.uint8),
            shape=(1, 224, 224),
        )
        pixel_values = self.image_processor(img, return_tensors="pt").pixel_values
        
        chunks = self.text_splitter.split_text(text)
        
        outputs = []
        for chunk in chunks:
            inputs = self.tokenizer(chunk, **self.tokenizer_options)
            global_attention_mask = [
                [1 if token_id == self.tokenizer.cls_token_id else 0 for token_id in input_ids]
                for input_ids in inputs["input_ids"]
            ]
            inputs["global_attention_mask"] = torch.tensor(global_attention_mask)
            
            model_input = dict(
                longformer_input_ids=inputs['input_ids'],
                longformer_attention_mask=inputs['attention_mask'],
                longformer_global_attention_mask=inputs['global_attention_mask'],
                resnet_pixel_values=pixel_values
            )
            model_input = {key: value.to(self.device) for key, value in model_input.items()}

            max_elements, max_indices = (
                self.model(**model_input)
                    .cpu()
                    .softmax(-1)
                    .max(-1)
            )
            outputs.append(
                {
                    'label_id': int(max_indices[0]),
                    'score': float(max_elements[0])
                }
            )
        
        malware_outputs = []
        benign_outputs = []
        for item in outputs:
            if item['label_id'] == 0:
                benign_outputs.append(item)
            elif item['label_id'] == 1:
                malware_outputs.append(item)
            else:
                raise NotImplementedError()
        
        if malware_outputs:
            return max(malware_outputs, key=lambda item: item['score'])
        
        return max(benign_outputs, key=lambda item: item['score'])

    